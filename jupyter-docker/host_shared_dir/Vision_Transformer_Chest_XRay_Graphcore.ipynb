{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af957ecd",
   "metadata": {},
   "source": [
    "This notebook will demonstrate the process of using HuggingFace Optimum to train ViT on the ChestX-ray14 Dataset, but you can plug and play any dataset for a quicker time to value for your AI projects.\n",
    "\n",
    "In order to streamline your experience, we have created some simple scripts \n",
    "which aims to minimise the demo setup time and even make the demo accessible \n",
    "to users with minimal experience of using IPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Getting the dataset\n",
    "\n",
    "Begin by downloading the Chest Xray Dataset. The dataset contains 112,120 frontal view X-rays of 30805 people who had common diseases recorded between 1992 - 2015 with 14 labels mined from the radiological report text using NLP techniques.\n",
    "\n",
    "![xray-sample.jpeg](static/xray-sample.jpeg)\n",
    "\n",
    "Download dataset at https://nihcc.app.box.com/v/ChestXray-NIHCC\n",
    "\n",
    "Extract all files:\n",
    "```\n",
    "for f in images*.tar.gz; do tar xfz \"$f\"; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_Entry_2017_v2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a63e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610783a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Finding Labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb864be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to have spaces in folder names\n",
    "\n",
    "data['Finding Labels'] = data['Finding Labels'].str.replace('No Finding', 'No_Finding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images have multiple labels\n",
    "# We split them into columns\n",
    "\n",
    "findings = data['Finding Labels'].str.split('|', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images have multiple labels, we keep only the first one\n",
    "\n",
    "data['Finding Labels'] = findings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d45b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Finding Labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Finding Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('processed_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['Finding Labels'].unique()\n",
    "for l in labels:\n",
    "    os.mkdir('processed_images/{}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy each image to its label subfolder\n",
    "\n",
    "for image, label in zip(data['Image Index'], data['Finding Labels']):\n",
    "    shutil.copy('images/{}'.format(image), 'processed_images/{}/'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d7df1",
   "metadata": {},
   "source": [
    "# Training and evaluation\n",
    "\n",
    "You can also train with \n",
    "https://github.com/huggingface/optimum-graphcore/tree/main/examples/image-classification\n",
    "    \n",
    "```\n",
    "python run_image_classification.py \\\n",
    "    --model_name_or_path google/vit-base-patch16-224-in21k \\\n",
    "    --ipu_config_name Graphcore/vit-base-ipu \\\n",
    "    --train_dir processed_images/ \\\n",
    "    --train_val_split 0.1 \\\n",
    "    --output_dir ./outputs/ \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    --dataloader_drop_last \\\n",
    "    --seed 1337\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b0bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "data_dir = \"processed_images\"\n",
    "validation_dir = None\n",
    "train_val_split = 0.1\n",
    "model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
    "ipu_config_name = \"Graphcore/vit-base-ipu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b861dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a7a7ceb31245589e89818bd0caa40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/112120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9e135e5054dbcf0c\n",
      "Reusing dataset image_folder (/home/jincheng/.cache/huggingface/datasets/image_folder/default-9e135e5054dbcf0c/0.0.0/48efdc62d40223daee675ca093d163bcb6cb0b7d7f93eb25aebf5edca72dc597)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b579f429cefb4e3b901da6dd28a019e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jincheng/.cache/huggingface/datasets/image_folder/default-9e135e5054dbcf0c/0.0.0/48efdc62d40223daee675ca093d163bcb6cb0b7d7f93eb25aebf5edca72dc597/cache-7019811ef7abccdb.arrow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=data_dir,\n",
    "    task=\"image-classification\",\n",
    ")\n",
    "\n",
    "split = dataset[\"train\"].train_test_split(train_val_split)\n",
    "dataset[\"train\"] = split[\"train\"]\n",
    "dataset[\"validation\"] = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf0c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare label mappings.\n",
    "# We'll include these in the model's config to get human readable labels in the Inference API.\n",
    "labels = dataset[\"train\"].features[\"labels\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf7d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy metric from the datasets package\n",
    "import datasets\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "metric_acc = datasets.load_metric(\"accuracy\")\n",
    "metric_auc = datasets.load_metric(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "# Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = metric_acc.compute(predictions=preds, references=p.label_ids)['accuracy']\n",
    "    \n",
    "    pred_scores = softmax(p.predictions.astype('float32'), axis=1)\n",
    "    auc = metric_auc.compute(prediction_scores=pred_scores, references=p.label_ids, multi_class='ovo')['roc_auc']\n",
    "    return {\"accuracy\": acc, \"roc_auc\": auc}\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     my_predictions = np.zeros_like(p.predictions)\n",
    "#     my_predictions[:, 10] = 1\n",
    "    \n",
    "#     preds = np.argmax(my_predictions, axis=1)\n",
    "#     acc = metric_acc.compute(predictions=preds, references=p.label_ids)['accuracy']\n",
    "    \n",
    "#     pred_scores = softmax(my_predictions.astype('float32'), axis=1)\n",
    "#     auc = metric_auc.compute(prediction_scores=pred_scores, references=p.label_ids, multi_class='ovo')['roc_auc']\n",
    "#     return {\"accuracy\": acc, \"roc_auc\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2032fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "from optimum.graphcore import IPUConfig\n",
    "\n",
    "\n",
    "ipu_config = IPUConfig.from_pretrained(\n",
    "    ipu_config_name\n",
    ")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define torchvision transforms to be applied to each image.\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        Resize(feature_extractor.size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize(feature_extractor.size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811290e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement transforms as a functor instead of a function because the Async Dataloader\n",
    "# can't handle functions with closures because it uses pickle underneath.\n",
    "class ApplyTransforms:\n",
    "    \"\"\"\n",
    "    Functor that applies image transforms across a batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, example_batch):\n",
    "        example_batch[\"pixel_values\"] = [self.transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"]]\n",
    "        return example_batch\n",
    "\n",
    "# Set the training transforms\n",
    "dataset[\"train\"].set_transform(ApplyTransforms(_train_transforms))\n",
    "# Set the validation transforms\n",
    "dataset[\"validation\"].set_transform(ApplyTransforms(_val_transforms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3516f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e565ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting replicated_tensor_sharding to False when replication_factor=1\n",
      "---------- Device Allocation -----------\n",
      "Embedding  --> IPU 0\n",
      "Encoder 0  --> IPU 0\n",
      "Encoder 1  --> IPU 0\n",
      "Encoder 2  --> IPU 0\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 1\n",
      "Encoder 5  --> IPU 1\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 2\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Head       --> IPU 3\n",
      "---------------------------------------\n",
      "Compiling Model...\n",
      "/localdata/jincheng/sdks/poplar_sdk-ubuntu_18_04-2.6.0-EA.1+1013-351d0fa429/2.6.0-EA.1+1013_poptorch/lib/python3.6/site-packages/transformers/models/vit/modeling_vit.py:186: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n",
      "Graph compilation: 100%|██████████| 100/100 [00:13<00:00]\n",
      "Compiled/Loaded model in 58.4291013404727 secs\n",
      "***** Running training *****\n",
      "  Num examples = 100908\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Device Iterations = 1\n",
      "  Replication Factor = 1\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Total optimization steps = 2364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msw-apps\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.17 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./results</strong> to <a href=\"https://wandb.sourcevertex.net\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface\" target=\"_blank\">https://wandb.sourcevertex.net/sw-apps/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface/runs/32h1v6kx\" target=\"_blank\">https://wandb.sourcevertex.net/sw-apps/huggingface/runs/32h1v6kx</a><br/>\n",
       "                Run data is saved locally in <code>/localdata/jincheng/datasets/CXR8/wandb/run-20220609_120758-32h1v6kx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f67448027c43018bea9def667653ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9474, 'learning_rate': 4.89424703891709e-05, 'epoch': 0.06}\n",
      "{'loss': 1.366, 'learning_rate': 4.7884940778341796e-05, 'epoch': 0.13}\n",
      "{'loss': 1.6601, 'learning_rate': 4.682741116751269e-05, 'epoch': 0.19}\n",
      "{'loss': 1.5776, 'learning_rate': 4.576988155668359e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4641, 'learning_rate': 4.471235194585449e-05, 'epoch': 0.32}\n",
      "{'loss': 1.6877, 'learning_rate': 4.365482233502538e-05, 'epoch': 0.38}\n",
      "{'loss': 1.8744, 'learning_rate': 4.259729272419628e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4432, 'learning_rate': 4.153976311336718e-05, 'epoch': 0.51}\n",
      "{'loss': 1.341, 'learning_rate': 4.0482233502538075e-05, 'epoch': 0.57}\n",
      "{'loss': 1.8149, 'learning_rate': 3.942470389170897e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4903, 'learning_rate': 3.836717428087986e-05, 'epoch': 0.7}\n",
      "{'loss': 1.7573, 'learning_rate': 3.7309644670050766e-05, 'epoch': 0.76}\n",
      "{'loss': 1.1495, 'learning_rate': 3.6252115059221656e-05, 'epoch': 0.82}\n",
      "{'loss': 1.4042, 'learning_rate': 3.519458544839256e-05, 'epoch': 0.89}\n",
      "{'loss': 1.484, 'learning_rate': 3.413705583756345e-05, 'epoch': 0.95}\n",
      "{'loss': 1.7076, 'learning_rate': 3.307952622673435e-05, 'epoch': 1.02}\n",
      "{'loss': 1.2944, 'learning_rate': 3.202199661590525e-05, 'epoch': 1.08}\n",
      "{'loss': 1.0996, 'learning_rate': 3.096446700507614e-05, 'epoch': 1.14}\n",
      "{'loss': 1.2906, 'learning_rate': 2.990693739424704e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3619, 'learning_rate': 2.8849407783417938e-05, 'epoch': 1.27}\n",
      "{'loss': 1.2484, 'learning_rate': 2.7791878172588832e-05, 'epoch': 1.33}\n",
      "{'loss': 1.6962, 'learning_rate': 2.6734348561759732e-05, 'epoch': 1.4}\n",
      "{'loss': 1.4577, 'learning_rate': 2.5676818950930626e-05, 'epoch': 1.46}\n",
      "{'loss': 1.6515, 'learning_rate': 2.4619289340101523e-05, 'epoch': 1.52}\n",
      "{'loss': 1.5861, 'learning_rate': 2.356175972927242e-05, 'epoch': 1.59}\n",
      "{'loss': 1.2375, 'learning_rate': 2.2504230118443317e-05, 'epoch': 1.65}\n",
      "{'loss': 1.5742, 'learning_rate': 2.1446700507614213e-05, 'epoch': 1.71}\n",
      "{'loss': 1.3853, 'learning_rate': 2.038917089678511e-05, 'epoch': 1.78}\n",
      "{'loss': 1.1842, 'learning_rate': 1.9331641285956007e-05, 'epoch': 1.84}\n",
      "{'loss': 1.3253, 'learning_rate': 1.8274111675126904e-05, 'epoch': 1.9}\n",
      "{'loss': 1.2229, 'learning_rate': 1.72165820642978e-05, 'epoch': 1.97}\n",
      "{'loss': 1.3848, 'learning_rate': 1.6159052453468698e-05, 'epoch': 2.03}\n",
      "{'loss': 1.4991, 'learning_rate': 1.5101522842639595e-05, 'epoch': 2.09}\n",
      "{'loss': 1.0086, 'learning_rate': 1.404399323181049e-05, 'epoch': 2.16}\n",
      "{'loss': 1.2611, 'learning_rate': 1.2986463620981387e-05, 'epoch': 2.22}\n",
      "{'loss': 1.244, 'learning_rate': 1.1928934010152284e-05, 'epoch': 2.28}\n",
      "{'loss': 1.0634, 'learning_rate': 1.0871404399323181e-05, 'epoch': 2.35}\n",
      "{'loss': 1.5934, 'learning_rate': 9.813874788494078e-06, 'epoch': 2.41}\n",
      "{'loss': 1.1753, 'learning_rate': 8.756345177664975e-06, 'epoch': 2.47}\n",
      "{'loss': 1.3863, 'learning_rate': 7.698815566835872e-06, 'epoch': 2.54}\n",
      "{'loss': 1.3332, 'learning_rate': 6.641285956006768e-06, 'epoch': 2.6}\n",
      "{'loss': 1.3349, 'learning_rate': 5.583756345177665e-06, 'epoch': 2.66}\n",
      "{'loss': 1.359, 'learning_rate': 4.526226734348562e-06, 'epoch': 2.73}\n",
      "{'loss': 1.3915, 'learning_rate': 3.4686971235194584e-06, 'epoch': 2.79}\n",
      "{'loss': 1.121, 'learning_rate': 2.4111675126903553e-06, 'epoch': 2.86}\n",
      "{'loss': 1.2436, 'learning_rate': 1.353637901861252e-06, 'epoch': 2.92}\n",
      "{'loss': 1.183, 'learning_rate': 2.9610829103214894e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 875.4389, 'train_samples_per_second': 345.797, 'train_steps_per_second': 2.7, 'train_loss': 1.4113887522224646, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2364, training_loss=1.4113887522224646, metrics={'train_runtime': 875.4389, 'train_samples_per_second': 345.797, 'train_steps_per_second': 2.7, 'train_loss': 1.4113887522224646, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.graphcore import IPUTrainer\n",
    "from optimum.graphcore import IPUTrainingArguments as TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_drop_last=True,\n",
    "    num_train_epochs=3,\n",
    "#     max_steps = 10,\n",
    "    seed=1337,\n",
    "    logging_steps=50,\n",
    "    save_steps=5000,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85507e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:05<00:00]\n",
      "Compiled/Loaded model in 43.46780707128346 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11212\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747077a96927497fa400c7a4ff647a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2803 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.5718\n",
      "  eval_loss               =      1.376\n",
      "  eval_roc_auc            =     0.7208\n",
      "  eval_runtime            = 0:00:40.28\n",
      "  eval_samples_per_second =    278.319\n",
      "  eval_steps_per_second   =      69.58\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab482f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89d0d14334cf640b941c287d32417307995983d41a0a4ac5ca10abead6b1abe3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
